# Распределенное Key-Value хранилище с консенсусом на основе алгоритма Raft

## Описание системы
Распределенное отказоустойчивое Key-Value хранилище, требующее консенсуса на основе алгоритма Raft. Основная цель — обеспечить надёжное хранилище данных с автоматическим восстановлением после сбоев и согласованием данных между несколькими узлами-кластера с помощью механизма выбора лидера, логов и репликаций, как это предписывает протокол Raft. Система способна к восстановлению без потери данных в случае гибели узлов. Все данные хранится в памяти. Язык разработки - Java.

### Система поддерживает:
- Логирование команд (`LogEntry`), обеспечение согласованности между узлами (`AppendEntries`, `RequestVote` и др.)
- Хранилище ключ-значение (key-value) с минимальной бизнес-логикой (`KeyValueStateMachine`)
- Обмен сообщениями между узлами через HTTP (`HttpRaftTransport`)
- Запуск серверного REST-интерфейса (`RaftHttpServer`)
- Конфигурация кластера (`ClusterConfig`, `PeerEndpoint`)

### Основные реализованные идеи Raft
- **Выбор лидера:** В каждый момент времени есть только один лидер, который принимает клиентские запросы и инициирует запись в журнал (лог).
- **Репликация лога:** Лидер записывает команды в свой лог и синхронно отправляет их на остальные узлы (реплики).
- **Согласование:** Команда считается подтверждённой, когда её приняли большинство узлов.
- **Безопасность:** Только команды, записанные на большинстве узлов, применяются к состоянию системы.
- **Перевыборы лидера:** Если текущий лидер недоступен, остальные узлы проводят новый выбор на основании голосования и номеров логов.
  
### Архитектура
- Узлы кластера:
  - Каждая нода хранит собственный лог команд, принимает команды клиентов и участвует в выборах лидера.
  - Нода принимает одну из трех ролей: Leader (лидер), Follower (ведущийся) и Candidate (кандидат).
- Состояния и переходы:
  - Начальное состояние узла — follower.
  - Если follower долго не получает heartbeat, запускается случайно установленный таймер выборов. Узел становится кандидатом, увеличивает свой термин и отправляет всем RequestVote RPC.
  - Другие узлы дают голос только если:
    - Не голосовали в этом термине, либо голосуют за того же кандидата.
    - Лог кандидата как минимум такой же актуальный, как у них.
  - Победивший в выборах кандидат(набравший большинсво голосов) становится лидером до конца термина. Лидер регулярно отправляет heartbeat (пустые AppendEntries) для поддержания своей власти.
  - Смена роли ноды на follower происходит при получении AppendEntries или RequestVote с большим term. Нода после этого сохраняет ID лидера, сбрасывает таймер выборов.
- Обработка команд клиента
  - Клиент отправляет команду лидеру.
  - Лидер добавляет команду как новую запись в свой лог и инициирует асинхронную репликацию этой записи остальным узлам (broadcastReplications).
  - Только после того, как большинство подтвердит запись, команда считается закоммиченной, и лидер применяет её к состоянию (stateMachine). Затем результат отправляется клиенту.
- Механизм репликации
  - Репликации реализованы через AppendEntries RPC: лидер отправляет новые или “догоняющие” записи follower-нодам.
  - Ноды проверяют консистентность лога: если терм/индекс не совпадает — отклоняют запрос, если запись принята, обновляют свой лог и подтверждают запись.

## Структура проекта

- `docker-compose.yml`, `Dockerfile` — контейнеризация и кластерный запуск  
- `pom.xml` — Maven-конфигурация

- `src/main/java/org/example/`
  - `kv/`
    - `KeyValueCommand.java` — команды для KV-хранилища (PUT, GET, DELETE)
    - `KeyValueResult.java` — результат выполнения команд
    - `KeyValueStateMachine.java` — бизнес-логика state machine
  - `raft/`
    - `RaftNode.java` — основной узел Raft
    - `RaftState.java` — внутренние состояния узла
    - `StateMachine.java` — интерфейс state machine
    - `NotLeaderException.java` — ошибка для не-лидера
    - `log/`
      - `LogEntry.java` — лог записи Raft
    - `protocol/`
      - `AppendEntriesRequest.java`, `AppendEntriesResponse.java` — синхронизация лога
      - `RequestVoteRequest.java`, `RequestVoteResponse.java` — голосования при выборах
    - `cluster/`
      - `ClusterConfig.java` — конфигурация кластера
      - `PeerEndpoint.java` — адресация узлов
    - `transport/`
      - `RaftTransport.java` — абстракция транспорта
      - `HttpRaftTransport.java` — HTTP-транспорт
    - `util/`
      - `Json.java` — утилиты сериализации
  - `server/`
    - `RaftHttpServer.java` — HTTP-сервер кластера
  - `Main.java` — точка входа
  - `src/test/java/org/example/`
    - `kv/`
       - `KeyValueStateMachineTest.java` — тесты state machine
---

### API
- `GET /raft/status` - получение статуса узла
- `POST /kv/put` — тело `{ "key": "...", "value": "..." }` // запись пары ключ-значение
- `POST /kv/delete` — тело `{ "key": "..." }` // удаление значения по ключу
- `GET /kv/get?key=...` // чтение значения по ключу

Обращаться следует к лидеру. Фолловер(follower) вернёт HTTP 409 с подсказкой `leader`.

### Запуск кластера с Docker

Все узлы кластера упакованы в отдельные Docker контейнеры.

```bash
# Сборка образов и запуск кластера
docker-compose up --build

# Запуск в фоновом режиме
docker-compose up -d --build

# Просмотр логов
docker-compose logs -f

# Просмотр логов конкретного узла
docker-compose logs -f node1

# Остановка кластера
docker-compose down
```

Узлы доступны на портах:
- `node1`: http://localhost:9001
- `node2`: http://localhost:9002
- `node3`: http://localhost:9003
- `node4`: http://localhost:9004

---

## Тестирование
### Сценарии тестирования:
1. **Начало работы** - проверка запуска кластера и выбора лидера.
2. **Репликация** - проверка репликации данных лидера на фолловеров (followers).
3. **Выживание большинства** - проверка работоспособности, если отказал 1 узел из 4.
4. **Граничный случай** - проверка "отказа" кластера, если отказали 2 узла из 2 (отсутствие кворума).
5. **Отсутствие кворума** - проверка "отказа" кластера, если доступно меньше половины узлов (1 из 4).
6. **Отказ лидера** - проверка перевыбора лидера в случае его недоступности.
7. **Возвращение лидера** - проверка синхронизации лидера с текущим состоянием кластера после его восстановления.
8. **Возвращение ноды** - проверка синхронизации обычной ноды(follower) с текущим состоянием кластера после ее восстановления.
9. **Персистентность данных** - проверка целостности и доступности данных после смены лидера.

Тестирование проводилось с помощью скрипта `raft_integration_test.py`. По результатам тестирования сформировон отчет `raft_integration_test_report.md`. Тесты показали корректную работу кластера по всем проверяемым сценариям.


